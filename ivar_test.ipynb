{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e9fa691",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46d3b066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Er', 'ontstaan', 'gezwellen', '(tumoren).', 'Hoewel', 'het', 'woord', \"'tumor'\", 'voor', 'patiënten', 'vaak', 'een', 'angstige', 'bijklank', 'heeft', 'betekent', 'het', 'niet', 'meer', 'of', 'minder', 'dan', \"'zwelling'.\", 'Een', 'tumor', 'kan', 'zowel', 'goed-', 'als', 'kwaadaardig', 'zijn.', 'Een', 'goedaardige', 'tumor', 'wordt', 'ook', 'wel', 'benigne', 'genoemd,', 'een', 'kwaadaardige', 'maligne.', 'Bij', 'kanker', 'is', 'er', 'sprake', 'van', 'maligne', 'tumoren.', 'Kankerweefsel', 'geneest', 'niet', 'goed', 'en', 'gaat', 'makkelijk', 'bloeden.', 'Bloedverlies,', 'bijvoorbeeld', 'bij', 'ontlasting,', 'urine,', 'uit', 'de', 'tepel', 'of', 'bij', 'hoesten,', 'is', 'een', 'van', 'de', 'belangrijke', 'vroege', 'waarschuwingssymptomen.', 'De', 'gezwellen', 'drukken', 'op', 'andere', 'structuren', 'en', 'belemmeren', 'daarvan', 'de', 'werking.', 'Bij', 'de', 'darm', 'kan', 'bijvoorbeeld', 'passage', 'van', 'voedsel', 'onmogelijk', 'worden;', 'vanuit', '-', 'door', 'zwellingen', 'geblokkeerde', '-', 'zenuwbanen', 'in', 'het', 'ruggenmerg', 'kunnen', 'verlammingen', 'ontstaan;', 'in', 'botten', 'kunnen', 'breuken', 'optreden;', 'bij', 'zenuwen', 'kan', 'pijn', 'ontstaan;', 'bij', 'hersentumoren', 'ontstaan', 'er', 'ook', 'andere', 'neurologische', 'problemen', 'zoals', 'epilepsie.', 'Als', 'het', 'beenmerg', '-', 'waar', 'de', 'bloedaanmaak', 'plaatsvindt', '-', 'door', 'tumorweefsel', 'wordt', 'vervangen,', 'ontstaan', 'ernstige', 'bloedarmoede', 'en', 'stollingsstoornissen.', 'Kanker', 'veroorzaakt', 'vaak', 'verandering', 'van', 'de', 'stofwisseling', 'en', 'regulatie', 'daarvan', '(paraneoplastische', 'syndromen),', 'waaronder:', 'verhoogde', 'hormoonproductie;', 'hersen-,', 'zenuw-', 'en/of', 'spierafwijkingen;', 'bloed-', 'en', 'stollingsafwijkingen;', 'huidafwijkingen;', '', '', '', 'koorts', '(tumorkoorts);', 'cachexie', '(vermagering),', 'anorexie', '(verminderde', 'eetlust).']\n"
     ]
    }
   ],
   "source": [
    "from tokenizer_ivar import *\n",
    "x = filereader(\"test.txt\")\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "35351615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'E', 2: 'r', 3: 'o', 4: 'n', 5: 't', 6: 's', 7: 'a', 8: 'g', 9: 'e', 10: 'z', 11: 'w', 12: 'l', 13: '(', 14: 'u', 15: 'm', 16: ')', 17: '.', 18: 'H', 19: 'h', 20: 'd', 21: \"'\", 22: 'v', 23: 'p', 24: 'i', 25: 'ë', 26: 'k', 27: 'b', 28: 'j', 29: 'f', 30: '-', 31: ',', 32: 'B', 33: 'K', 34: 'c', 35: 'y', 36: 'D', 37: ';', 38: 'A', 39: ':', 40: '/', 41: 'x', 42: 'en', 43: 'an', 44: 'er', 45: 'or', 46: 'in', 47: 'el', 48: 'st', 49: 'ij', 50: 'aa', 51: 'oe', 52: 'ing', 53: 'oor', 54: 'oed'}\n"
     ]
    }
   ],
   "source": [
    "def encoder(word_list, max_dict_len, min_freqwentie):\n",
    "    token_dict = {}\n",
    "    token_value_dict = {}\n",
    "    counter = 1\n",
    "\n",
    "    # start vocab\n",
    "    for word_index in range(len(word_list)):\n",
    "        current_word = word_list[word_index]\n",
    "        for letter_index in range(len(current_word)):\n",
    "            letter = current_word[letter_index]\n",
    "            if token_value_dict.get(letter) == None:\n",
    "                token_value_dict[letter] = 1\n",
    "                token_dict[letter] = counter\n",
    "                counter += 1\n",
    "            else:\n",
    "                current_value = token_value_dict.get(letter)\n",
    "                new_value = current_value + 1\n",
    "                token_value_dict[letter] = new_value\n",
    "    # tokenize original words 1 toker per letter\n",
    "    words_in_tokens = []\n",
    "    for word_index in range(len(word_list)):\n",
    "        current_word = word_list[word_index]\n",
    "        word_in_tokens = []\n",
    "        for letter_index in range(len(current_word)):\n",
    "            current_letter = current_word[letter_index]\n",
    "            new_token = token_dict.get(current_letter)\n",
    "            word_in_tokens.append(new_token)\n",
    "        words_in_tokens.append(word_in_tokens)\n",
    "    # reverse key value\n",
    "    reversed_token_dict = {}\n",
    "    for key,value in token_dict.items():\n",
    "        new_key = value\n",
    "        new_value = key\n",
    "        reversed_token_dict[new_key] = new_value\n",
    "\n",
    "\n",
    "    def pair_merger(token_lists, tokens_dict):\n",
    "        new_token_lists = []\n",
    "        temp_dict = {}\n",
    "        pair_dict = {}\n",
    "        for token_list_index in range(len(token_lists)):\n",
    "            current_token_list = token_lists[token_list_index]\n",
    "            if len(current_token_list) < 2:\n",
    "                continue\n",
    "            else:\n",
    "                end_token_index = 1\n",
    "                token_list_range = len(current_token_list)-1\n",
    "                for token_index in range(token_list_range):\n",
    "\n",
    "                    first_token_string = tokens_dict.get(current_token_list[token_index])\n",
    "                    first_token = current_token_list[token_index]\n",
    "                    secon_token_string = tokens_dict.get(current_token_list[end_token_index])\n",
    "                    second_token = current_token_list[end_token_index]\n",
    "                    end_token_index += 1\n",
    "\n",
    "                    candidate = first_token_string + secon_token_string\n",
    "                    if temp_dict.get(candidate) == None:\n",
    "                        temp_dict[candidate] = 1\n",
    "                        pair_dict[candidate] = [first_token,second_token]\n",
    "                    else:\n",
    "                        current_value = temp_dict.get(candidate)\n",
    "                        new_value = current_value + 1\n",
    "                        temp_dict[candidate] = new_value\n",
    "\n",
    "        key_list = []\n",
    "        value_list = []\n",
    "        for key,value in temp_dict.items():\n",
    "            key_list.append(key)\n",
    "            value_list.append(value)\n",
    "\n",
    "        value_check = max(value_list)\n",
    "        if value_check <min_freqwentie:\n",
    "            return None, None\n",
    "        \n",
    "        higest_value_index = value_list.index(max(value_list))\n",
    "        new_string_key = key_list[higest_value_index]\n",
    "\n",
    "        old_key_list = []\n",
    "        for key in tokens_dict.keys():\n",
    "            old_key_list.append(key)\n",
    "\n",
    "        new_token_key = max(old_key_list)+1\n",
    "        tokens_dict[new_token_key] = new_string_key\n",
    "        new_pair = pair_dict.get(new_string_key)\n",
    "        \n",
    "        for token_list_index in range(len(token_lists)):\n",
    "            current_token_list = token_lists[token_list_index]\n",
    "            temp_token_list = current_token_list.copy()\n",
    "            end_index = 1\n",
    "            for token_index in range(len(current_token_list)-1):\n",
    "                first_token = current_token_list[token_index]\n",
    "                second_token = current_token_list[end_index]\n",
    "\n",
    "                if first_token is new_pair[0] and second_token is new_pair[1]:\n",
    "                    temp_token_list[token_index] = new_token_key\n",
    "                    temp_token_list[end_index] = None\n",
    "                    end_index += 1\n",
    "                else:\n",
    "                    end_index += 1\n",
    "            # remove nonetypes\n",
    "            if None in (temp_token_list):\n",
    "                temp_token_list.remove(None)\n",
    "                while None in (temp_token_list):\n",
    "                    temp_token_list.remove(None)\n",
    "            new_token_lists.append(temp_token_list)\n",
    "\n",
    "        return new_token_lists, tokens_dict\n",
    "\n",
    "    result_token_list = words_in_tokens.copy()\n",
    "    result_token_dict = reversed_token_dict.copy()\n",
    "    max_len = 10\n",
    "    # condense all words into tokens\n",
    "    while max_len >= 2:\n",
    "        if len(result_token_dict) == max_dict_len:\n",
    "            break\n",
    "        \n",
    "        temp_list, temp_dict = pair_merger(result_token_list,reversed_token_dict)\n",
    "        if temp_list is None:\n",
    "            break\n",
    "\n",
    "        result_token_list = temp_list\n",
    "        result_token_dict = temp_dict\n",
    "        temp = []\n",
    "        for i in range(len(result_token_list)):\n",
    "            current_token_list = result_token_list[i]\n",
    "            temp.append(len(current_token_list))\n",
    "        \n",
    "        max_len = max(temp)\n",
    "    \n",
    "\n",
    "    return result_token_list,result_token_dict\n",
    "\n",
    "new_tokens, new_dict = encoder(x,101,10)\n",
    "print(new_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1c916d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Encwriter(token_dict):\n",
    "    with open(\"result.enc\",\"w\") as writer:\n",
    "        for key,value in token_dict.items():\n",
    "            line = str(key)+\":\"+str(value)+\"\\n\"\n",
    "            writer.write(line)\n",
    "    return\n",
    "\n",
    "Encwriter(new_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[11], [10], [7]],\n",
       " {1: 'e',\n",
       "  2: 't',\n",
       "  3: 'l',\n",
       "  4: 'u',\n",
       "  5: 's',\n",
       "  6: 'ee',\n",
       "  7: 'eet',\n",
       "  8: 'lu',\n",
       "  9: 'lus',\n",
       "  10: 'lust',\n",
       "  11: 'eetlust'})"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = \"eetlust lust eet\"\n",
    "encoder(test.split())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
