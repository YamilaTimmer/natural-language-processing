{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Les 4 Embeddings\n",
    "\n",
    "In dit notebook worden er meerdere teksten (Wikipedia-pagina over kanker, abstracts en Bulkley-publicatie) verwerkt tot één dataset, wordt er tokenisatie uitgevoerd, wordt een MLP getraind om embeddings te leren, en worden deze embeddings in 2D gevisualiseerd.\n",
    "\n",
    "Er wordt geprobeerd om\n",
    "- Samengevoegde tekst te maken\n",
    "- Tokenisatie en BPE encoding uit te voeren\n",
    "- MLP embeddings te trainen\n",
    "- 2D-visualisatie van embeddings te maken"
   ],
   "id": "47ed728a01b5b56b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Teksten samenvoegen zodat er één bestand is genaamd merged.txt die gebruikt wordt voor tokenisatie",
   "id": "4e84042defaf7ce2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T17:34:35.257478Z",
     "start_time": "2025-12-13T17:34:35.188873Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#lees alle txt data in en maak er een merged.txt file van\n",
    "files = [\"../resources/cancer_wiki.txt\", \"../resources/gutenberg_cancer.txt\", \"../resources/Pubmed_cancer.txt\"]\n",
    "with open(\"merged.txt\", \"w\", encoding=\"utf-8\") as fout:\n",
    "    for file in files:\n",
    "        with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "            fout.write(f.read() + \"\\n\")\n",
    "\n",
    "with open(\"merged.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    print(f.read(500))  # print eerste 500 tekens zodat je kunt zien dat merged.txt is aangemaakt\n"
   ],
   "id": "4b22b8d0a983d9c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cancer is a group of diseases involving abnormal cell growth with the potential to invade or spread to other parts of the body.[2][7] These contrast with benign tumors, which do not spread.[7] Possible signs and symptoms of cancer include a lump, abnormal bleeding, prolonged cough, unexplained weight loss, and a change in bowel movements.[1] While these symptoms may indicate cancer, they can also have other causes.[1] Over 100 types of cancers affect humans.[7][8]\n",
      "\n",
      "About 33% of deaths from cance\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Tokenisatie\n",
    "Woorden maken van de tekst, elk element is nog een woord en nog geen BPE-token"
   ],
   "id": "1025dd08a58815f0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T17:34:38.031482Z",
     "start_time": "2025-12-13T17:34:37.636858Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nlp import filereader\n",
    "\n",
    "# Woordenlijst maken door op witruimte te splitsen\n",
    "woorden = filereader(\"merged.txt\")\n",
    "print(f\"Aantal woorden in tekst: {len(woorden)}\")\n",
    "print(\"Voorbeeld woorden:\", woorden[:50])"
   ],
   "id": "a3ba0882042936f5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aantal woorden in tekst: 96702\n",
      "Voorbeeld woorden: ['cancer', 'is', 'a', 'group', 'of', 'diseases', 'involving', 'abnormal', 'cell', 'growth', 'with', 'the', 'potential', 'to', 'invade', 'or', 'spread', 'to', 'other', 'parts', 'of', 'the', 'body.[2][7]', 'these', 'contrast', 'with', 'benign', 'tumors,', 'which', 'do', 'not', 'spread.[7]', 'possible', 'signs', 'and', 'symptoms', 'of', 'cancer', 'include', 'a', 'lump,', 'abnormal', 'bleeding,', 'prolonged', 'cough,', 'unexplained', 'weight', 'loss,', 'and', 'a']\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### BPE/Encoder maken\n",
    "\n",
    "BPE-tokenisatie uitvoeren (Byte-pair encoding)\n",
    "Converteer woorden naar numerieke token-id's met token(string)."
   ],
   "id": "4c2414f57fbfbbb0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T17:50:11.284007Z",
     "start_time": "2025-12-13T17:48:06.192790Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nlp import encoder\n",
    "\n",
    "# BPE/encoder maken\n",
    "#max_tokens: maximale unieke tokens in de encoding\n",
    "#min_freq: minimaal aantal keren dat een paar tokens moet voorkomen om samengevoegd te worden\n",
    "\n",
    "# Eerste set instellingen\n",
    "words_tokens_1, id_to_tok_1 = encoder(woorden, max_tokens=120, min_freq=2)\n",
    "print(f\"[Run 1] Aantal unieke tokens: {len(id_to_tok_1)}\")\n",
    "\n",
    "# Tweede set instellingen\n",
    "words_tokens_2, id_to_tok_2 = encoder(woorden, max_tokens=200, min_freq=3)\n",
    "print(f\"[Run 2] Aantal unieke tokens: {len(id_to_tok_2)}\")\n",
    "\n",
    "# Derde set instellingen\n",
    "words_tokens_3, id_to_tok_3 = encoder(woorden, max_tokens=500, min_freq=5)\n",
    "print(f\"[Run 3] Aantal unieke tokens: {len(id_to_tok_3)}\")\n",
    "\n",
    "# Vierde set instellingen\n",
    "words_tokens_4, id_to_tok_4 = encoder(woorden, max_tokens=1000, min_freq=10)\n",
    "print(f\"[Run 4] Aantal unieke tokens: {len(id_to_tok_4)}\")\n",
    "\n",
    "# Toon token-ID's én de inhoud voor de eerste 5 woorden\n",
    "print(\"\\nVoorbeeld token-ID's en token-inhoud [Run 1]:\")\n",
    "for i, w_tokens in enumerate(words_tokens_1[:5]):\n",
    "    tokens_inhoud = [id_to_tok_1[t] for t in w_tokens]\n",
    "    print(f\"Woord {i+1}: token-ID's = {w_tokens}, tokens = {tokens_inhoud}\")\n",
    "\n",
    "print(\"\\nVoorbeeld token-ID's en token-inhoud [Run 2]:\")\n",
    "for i, w_tokens in enumerate(words_tokens_2[:5]):\n",
    "    tokens_inhoud = [id_to_tok_2[t] for t in w_tokens]\n",
    "    print(f\"Woord {i+1}: token-ID's = {w_tokens}, tokens = {tokens_inhoud}\")\n",
    "\n",
    "print(\"\\nVoorbeeld token-ID's en token-inhoud [Run 3]:\")\n",
    "for i, w_tokens in enumerate(words_tokens_3[:5]):\n",
    "    tokens_inhoud = [id_to_tok_3[t] for t in w_tokens]\n",
    "    print(f\"Woord {i+1}: token-ID's = {w_tokens}, tokens = {tokens_inhoud}\")\n",
    "\n",
    "print(\"\\nVoorbeeld token-ID's en token-inhoud [Run 4]:\")\n",
    "for i, w_tokens in enumerate(words_tokens_4[:5]):\n",
    "    tokens_inhoud = [id_to_tok_4[t] for t in w_tokens]\n",
    "    print(f\"Woord {i+1}: token-ID's = {w_tokens}, tokens = {tokens_inhoud}\")\n"
   ],
   "id": "68b00d5b3fef996e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Run 1] Aantal unieke tokens: 120\n",
      "[Run 2] Aantal unieke tokens: 200\n",
      "[Run 3] Aantal unieke tokens: 500\n",
      "[Run 4] Aantal unieke tokens: 1000\n",
      "\n",
      "Voorbeeld token-ID's en token-inhoud [Run 1]:\n",
      "Woord 1: token-ID's = [1, 100, 1, 97], tokens = ['c', 'an', 'c', 'er']\n",
      "Woord 2: token-ID's = [115], tokens = ['is']\n",
      "Woord 3: token-ID's = [2], tokens = ['a']\n",
      "Woord 4: token-ID's = [8, 5, 9, 10, 11], tokens = ['g', 'r', 'o', 'u', 'p']\n",
      "Woord 5: token-ID's = [110], tokens = ['of']\n",
      "\n",
      "Voorbeeld token-ID's en token-inhoud [Run 2]:\n",
      "Woord 1: token-ID's = [1, 100, 1, 97], tokens = ['c', 'an', 'c', 'er']\n",
      "Woord 2: token-ID's = [115], tokens = ['is']\n",
      "Woord 3: token-ID's = [2], tokens = ['a']\n",
      "Woord 4: token-ID's = [8, 130, 10, 11], tokens = ['g', 'ro', 'u', 'p']\n",
      "Woord 5: token-ID's = [110], tokens = ['of']\n",
      "\n",
      "Voorbeeld token-ID's en token-inhoud [Run 3]:\n",
      "Woord 1: token-ID's = [1, 100, 1, 97], tokens = ['c', 'an', 'c', 'er']\n",
      "Woord 2: token-ID's = [115], tokens = ['is']\n",
      "Woord 3: token-ID's = [2], tokens = ['a']\n",
      "Woord 4: token-ID's = [8, 130, 294], tokens = ['g', 'ro', 'up']\n",
      "Woord 5: token-ID's = [110], tokens = ['of']\n",
      "\n",
      "Voorbeeld token-ID's en token-inhoud [Run 4]:\n",
      "Woord 1: token-ID's = [478, 479], tokens = ['can', 'cer']\n",
      "Woord 2: token-ID's = [115], tokens = ['is']\n",
      "Woord 3: token-ID's = [2], tokens = ['a']\n",
      "Woord 4: token-ID's = [743, 294], tokens = ['gro', 'up']\n",
      "Woord 5: token-ID's = [110], tokens = ['of']\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Dataset bouwen voor MLP (multi-layer perceptrons)\n",
    "\n",
    "Bouwen van de input X en target Y met X = context vectors en Y = token-id's"
   ],
   "id": "b11be8df16f7d386"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T17:54:04.908799Z",
     "start_time": "2025-12-12T17:54:04.159534Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from nlp import build_token_mappings\n",
    "from embedding import build_dataset\n",
    "\n",
    "# Definieer je instellingen voor BPE-runs\n",
    "bpe_runs = [\n",
    "    {\"words_tokens\": words_tokens_1, \"id_to_tok\": id_to_tok_1, \"label\": \"Run 1\"},\n",
    "    {\"words_tokens\": words_tokens_2, \"id_to_tok\": id_to_tok_2, \"label\": \"Run 2\"},\n",
    "    {\"words_tokens\": words_tokens_3, \"id_to_tok\": id_to_tok_3, \"label\": \"Run 3\"},\n",
    "    {\"words_tokens\": words_tokens_4, \"id_to_tok\": id_to_tok_4, \"label\": \"Run 4\"}\n",
    "]\n",
    "\n",
    "# Kies de windows die je wilt testen\n",
    "windows = [2, 4, 6]\n",
    "\n",
    "# Opslag voor resultaten\n",
    "dataset_results = []\n",
    "\n",
    "for run in bpe_runs:\n",
    "    for window in windows:\n",
    "        # Maak token mappings\n",
    "        all_tokens, token_to_idx, idx_to_token = build_token_mappings(run[\"id_to_tok\"])\n",
    "        # Bouw dataset\n",
    "        X, Y, token_counter = build_dataset(run[\"words_tokens\"], run[\"id_to_tok\"], token_to_idx, n=window)\n",
    "        # Sla alles op in dictionary\n",
    "        result = {\n",
    "            \"label\": run[\"label\"],\n",
    "            \"window\": window,\n",
    "            \"all_tokens\": all_tokens,\n",
    "            \"token_to_idx\": token_to_idx,\n",
    "            \"idx_to_token\": idx_to_token,\n",
    "            \"X\": X,\n",
    "            \"Y\": Y,\n",
    "            \"token_counter\": token_counter\n",
    "        }\n",
    "        dataset_results.append(result)\n",
    "        print(f\"[{run['label']} | window={window}] Tokens: {len(all_tokens)}, Voorbeelden: {X.shape[0]}, Features: {X.shape[1]}\")\n"
   ],
   "id": "c965f52e1f528a2f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Run 1 | window=2] Tokens: 122, Voorbeelden: 163347, Features: 122\n",
      "[Run 1 | window=4] Tokens: 122, Voorbeelden: 32891, Features: 122\n",
      "[Run 1 | window=6] Tokens: 122, Voorbeelden: 3327, Features: 122\n",
      "[Run 2 | window=2] Tokens: 200, Voorbeelden: 52516, Features: 200\n",
      "[Run 2 | window=4] Tokens: 200, Voorbeelden: 3690, Features: 200\n",
      "[Run 2 | window=6] Tokens: 200, Voorbeelden: 730, Features: 200\n",
      "[Run 3 | window=2] Tokens: 500, Voorbeelden: 18374, Features: 500\n",
      "[Run 3 | window=4] Tokens: 500, Voorbeelden: 1585, Features: 500\n",
      "[Run 3 | window=6] Tokens: 500, Voorbeelden: 320, Features: 500\n",
      "[Run 4 | window=2] Tokens: 1000, Voorbeelden: 9568, Features: 1000\n",
      "[Run 4 | window=4] Tokens: 1000, Voorbeelden: 842, Features: 1000\n",
      "[Run 4 | window=6] Tokens: 1000, Voorbeelden: 122, Features: 1000\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Bij een hogere max_tokens maakt de BPE-encoder meer samengestelde tokens. Hierdoor worden de tokens langer en semantisch rijker. Omdat ieder token één feature in de one-hot vector vertegenwoordigt, stijgt de dimensionaliteit van de embedding-input wanneer max_tokens groter wordt.\n",
    "\n",
    "We zien ook dat het aantal trainingsvoorbeelden sterk daalt bij grotere window sizes.\n",
    "Voor Run 1 daalt het aantal voorbeelden van 163 347 (window = 2) naar 32 891 (window = 4) en uiteindelijk naar 3 327 (window = 6). Dit komt doordat voorbeelden alleen gemaakt kunnen worden wanneer er minimaal n contexttokens vóór en n contexttokens ná de target aanwezig zijn. Een grotere window reduceert dus drastisch het aantal bruikbare posities in de tekst.\n",
    "\n",
    "Daarnaast blijkt dat een grotere vocabulaire eveneens het aantal trainingsvoorbeelden verlaagt.\n",
    "Bijvoorbeeld: Run 1 met window = 2 heeft 163 347 voorbeelden, terwijl Run 4 met dezelfde window slechts 9 568 voorbeelden heeft. Dit gebeurt omdat meer BPE-merges leiden tot langere tokens; langere tokens betekenen minder totale tokens in de tekst, waardoor er minder contextvensters gevormd kunnen worden.\n",
    "\n",
    "Samengevat: een grote vocabulaire levert informatievere tokens op, maar verkleint de dataset. Een grote window levert rijkere contextinformatie op, maar verkleint de dataset nóg sterker.\n",
    "Daarom moet je zoeken naar een balans tussen voldoende trainingsdata, een passende vocabulairegrootte en een trainbare embeddingdimensie."
   ],
   "id": "5f07af0a08042e07"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Trainen MLP (multi-layer perceptrons) embeddings\n",
    "\n",
    "Trainen van een MLP op onze dataset\n",
    "> - `hidden_size` bepaalt dimensie van embedding\n",
    "> - `mlp.coefs_[0]` zijn de embeddings van de tokens"
   ],
   "id": "e9ac3efe04c385d9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-12T18:08:07.403861Z",
     "start_time": "2025-12-12T17:54:04.917078Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from embedding import train_mlp\n",
    "\n",
    "hidden_sizes = [10, 50, 80]  # verschillende hidden layer groottes\n",
    "mlp_results = []\n",
    "\n",
    "for hs in hidden_sizes:\n",
    "    print(f\"\\n--- Training MLP met hidden_size = {hs} ---\")\n",
    "    for data in dataset_results:\n",
    "        # Train MLP voor deze dataset\n",
    "        mlp = train_mlp(data[\"X\"], data[\"Y\"], hidden_size=hs)\n",
    "        embeddings = mlp.coefs_[0]  # input -> hidden laag\n",
    "\n",
    "        # Sla alles op\n",
    "        mlp_results.append({\n",
    "            \"label\": data[\"label\"],\n",
    "            \"window\": data[\"window\"],\n",
    "            \"hidden_size\": hs,\n",
    "            \"mlp\": mlp,\n",
    "            \"embeddings\": embeddings,\n",
    "            \"token_to_idx\": data[\"token_to_idx\"],\n",
    "            \"idx_to_token\": data[\"idx_to_token\"]\n",
    "        })\n",
    "\n",
    "        print(f\"[{data['label']} | window={data['window']} | hidden_size={hs}] Dimensie embeddings: {embeddings.shape}\")\n"
   ],
   "id": "3da3b1214c44911",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Training MLP met hidden_size = 10 ---\n",
      "[Run 1 | window=2 | hidden_size=10] Dimensie embeddings: (122, 10)\n",
      "[Run 1 | window=4 | hidden_size=10] Dimensie embeddings: (122, 10)\n",
      "[Run 1 | window=6 | hidden_size=10] Dimensie embeddings: (122, 10)\n",
      "[Run 2 | window=2 | hidden_size=10] Dimensie embeddings: (200, 10)\n",
      "[Run 2 | window=4 | hidden_size=10] Dimensie embeddings: (200, 10)\n",
      "[Run 2 | window=6 | hidden_size=10] Dimensie embeddings: (200, 10)\n",
      "[Run 3 | window=2 | hidden_size=10] Dimensie embeddings: (500, 10)\n",
      "[Run 3 | window=4 | hidden_size=10] Dimensie embeddings: (500, 10)\n",
      "[Run 3 | window=6 | hidden_size=10] Dimensie embeddings: (500, 10)\n",
      "[Run 4 | window=2 | hidden_size=10] Dimensie embeddings: (1000, 10)\n",
      "[Run 4 | window=4 | hidden_size=10] Dimensie embeddings: (1000, 10)\n",
      "[Run 4 | window=6 | hidden_size=10] Dimensie embeddings: (1000, 10)\n",
      "\n",
      "--- Training MLP met hidden_size = 50 ---\n",
      "[Run 1 | window=2 | hidden_size=50] Dimensie embeddings: (122, 50)\n",
      "[Run 1 | window=4 | hidden_size=50] Dimensie embeddings: (122, 50)\n",
      "[Run 1 | window=6 | hidden_size=50] Dimensie embeddings: (122, 50)\n",
      "[Run 2 | window=2 | hidden_size=50] Dimensie embeddings: (200, 50)\n",
      "[Run 2 | window=4 | hidden_size=50] Dimensie embeddings: (200, 50)\n",
      "[Run 2 | window=6 | hidden_size=50] Dimensie embeddings: (200, 50)\n",
      "[Run 3 | window=2 | hidden_size=50] Dimensie embeddings: (500, 50)\n",
      "[Run 3 | window=4 | hidden_size=50] Dimensie embeddings: (500, 50)\n",
      "[Run 3 | window=6 | hidden_size=50] Dimensie embeddings: (500, 50)\n",
      "[Run 4 | window=2 | hidden_size=50] Dimensie embeddings: (1000, 50)\n",
      "[Run 4 | window=4 | hidden_size=50] Dimensie embeddings: (1000, 50)\n",
      "[Run 4 | window=6 | hidden_size=50] Dimensie embeddings: (1000, 50)\n",
      "\n",
      "--- Training MLP met hidden_size = 80 ---\n",
      "[Run 1 | window=2 | hidden_size=80] Dimensie embeddings: (122, 80)\n",
      "[Run 1 | window=4 | hidden_size=80] Dimensie embeddings: (122, 80)\n",
      "[Run 1 | window=6 | hidden_size=80] Dimensie embeddings: (122, 80)\n",
      "[Run 2 | window=2 | hidden_size=80] Dimensie embeddings: (200, 80)\n",
      "[Run 2 | window=4 | hidden_size=80] Dimensie embeddings: (200, 80)\n",
      "[Run 2 | window=6 | hidden_size=80] Dimensie embeddings: (200, 80)\n",
      "[Run 3 | window=2 | hidden_size=80] Dimensie embeddings: (500, 80)\n",
      "[Run 3 | window=4 | hidden_size=80] Dimensie embeddings: (500, 80)\n",
      "[Run 3 | window=6 | hidden_size=80] Dimensie embeddings: (500, 80)\n",
      "[Run 4 | window=2 | hidden_size=80] Dimensie embeddings: (1000, 80)\n",
      "[Run 4 | window=4 | hidden_size=80] Dimensie embeddings: (1000, 80)\n",
      "[Run 4 | window=6 | hidden_size=80] Dimensie embeddings: (1000, 80)\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Het aantal hidden layers bepaalt alleen de dimensie van de embedding. Hidden_size veranderd niet hoeveel tokens er zijn of iets dergelijks maar alleen hoe groot de vector van elk token wordt. Dit experiment laat zien dat de embeddingdimensie uitsluitend wordt bepaald door de gekozen hidden_size, terwijl de vocabulairegrootte wordt bepaald door de BPE-instellingen (max_tokens). De window size beïnvloedt wél het aantal voorbeelden waaruit geleerd wordt, maar verandert niets aan de vorm van de embeddingmatrix.\n",
    "Daarom is:\n",
    "- hidden_size = 50 of 80 → betere embeddings dan 10\n",
    "- Run 2 (200 tokens) → beste balans tussen informatierijke tokens en genoeg data\n",
    "- window = 2 → enige window waarbij dataset niet instort"
   ],
   "id": "1f252c1b26435298"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Embeddings visualiseren\n",
    "Plot toont de liggen van de tokens in 2D\n",
    "> Gebruik `min_len` > 0 om alleen langere tokens te tonen.\n",
    "Bijvoorbeeld tokens van 5 karakters en langer min_len = 5"
   ],
   "id": "7505501cb458420d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T17:41:22.650200Z",
     "start_time": "2025-12-13T17:41:19.989155Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "for res in mlp_results:\n",
    "    embeddings = np.array(res[\"embeddings\"])\n",
    "    token_to_idx = res[\"token_to_idx\"]\n",
    "\n",
    "    # PCA naar 2D als embeddings meer dan 2 dimensies hebben\n",
    "    if embeddings.shape[1] > 2:\n",
    "        pca = PCA(n_components=2)\n",
    "        embeddings_2d = pca.fit_transform(embeddings)\n",
    "    else:\n",
    "        embeddings_2d = embeddings\n",
    "\n",
    "    # Plotten\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    for tok, idx in token_to_idx.items():\n",
    "        coord = embeddings_2d[idx][:2]  # neem eerste 2 dimensies\n",
    "        plt.scatter(coord[0], coord[1], color='blue')\n",
    "        plt.annotate(tok, (coord[0], coord[1]), fontsize=8)\n",
    "\n",
    "    # Dynamische titel\n",
    "    plt.title(f\"{res['label']} | window={res['window']} | hidden_size={res['hidden_size']}\")\n",
    "    plt.xlabel(\"Dimensie 1\")\n",
    "    plt.ylabel(\"Dimensie 2\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    plt.close()\n"
   ],
   "id": "60fc2ab56d03779f",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mlp_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNameError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[7]\u001B[39m\u001B[32m, line 5\u001B[39m\n\u001B[32m      2\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mmatplotlib\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01mpyplot\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mplt\u001B[39;00m\n\u001B[32m      3\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mnumpy\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mnp\u001B[39;00m\n\u001B[32m----> \u001B[39m\u001B[32m5\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m res \u001B[38;5;129;01min\u001B[39;00m \u001B[43mmlp_results\u001B[49m:\n\u001B[32m      6\u001B[39m     embeddings = np.array(res[\u001B[33m\"\u001B[39m\u001B[33membeddings\u001B[39m\u001B[33m\"\u001B[39m])\n\u001B[32m      7\u001B[39m     token_to_idx = res[\u001B[33m\"\u001B[39m\u001B[33mtoken_to_idx\u001B[39m\u001B[33m\"\u001B[39m]\n",
      "\u001B[31mNameError\u001B[39m: name 'mlp_results' is not defined"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Om te onderzoeken of de embeddings semantische relaties hebben geleerd, zoeken we voor elk token het dichtstbijzijnde token in de embeddingruimte (via cosine-similarity). Dit is vergelijkbaar met wat Word2Vec doet.\n",
    "\n",
    "Wanneer we dit toepassen op de embeddings die we hebben getraind bijvoorbeeld die van Run 2, window=2, die qua dataset het meest stabiel is zou je verwachten dat bepaalde tokens redelijk dicht bij elkaar liggen zoals \"cancer\" en canc\" of \"ancer\", of \"cell\" en \"cells\". Langere tokens bestaan uit beteknisvolle delen van een woord, deze zouden dan ook dicht bij elkaar moeten liggen omdat er een logische relatie is tussen deze tokens.\n",
    "\n",
    "Korte tokens komen vaak voor in veel verschillende contexten, ze zijn dus erg generiek. Het model kan voor deze korte tokens zoals 1 karakter geen duidelijke betekenisruimte leren. Ze hebben daarom vaak willekeurige buren en buren zonder duidelijke semantiek (relatie/betekenis).\n",
    "\n",
    "De beste resultaten kun je zien bij window 2, max_tokens tussen de 200 en 500 en een hidden_size van tussen de 50-80. Dit is het gebied waar tokens genoeg context krijgen om echte structuur te leren.\n",
    "\n",
    "### Conclusie\n",
    "Hoewel het notebook een functionele pipeline laat zien voor het trainen van token embeddings met een MLP en BPE zijn er een aantal punten waardoor het nog niet optimaal is:\n",
    "- Databeperking;\n",
    "\n",
    "De gebruikte dataset is relatief klein vergeleken met echte modellen waardoor de relaties van tokens beperkt zijn. Ook voor grotere window sizes of een groot aantal tokens neemt het aantal trainingsvoorbeelden sterk af, waardoor sommige embeddings minder betrouwbaar zijn.\n",
    "- Tokenisatie en BPE;\n",
    "\n",
    "De BPE-encoder maakt samengestelde tokens, maar sommige belangrijke contexten kunnen nog steeds niet goed worden vastgelegd. Ook hebben korte tokens vaak geen duidelijke betekenis in de embedding.\n",
    "\n",
    "- Modelbeperking;\n",
    "\n",
    "Het MLP is vooral gebruikt om te fungeren voor demonstraties, maar kan complexe relaties niet zo goed leren als diepere netwerken. Ook wordt de volgorde van tokens niet echt meegenomen.\n",
    "\n",
    "- Visualisatie\n",
    "\n",
    "De PCA-reductie naar 2 dimensies laat slechts een indruk zien van de embedding-structuur. Relaties in hogere dimensies kunnen verloren gaan. Ook zijn de plots chaotisch en onoverzichtelijk.\n"
   ],
   "id": "2460769171e5fa07"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
