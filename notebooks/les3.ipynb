{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Toepassing N-Gram Text Generator",
   "id": "b645aeef83969198"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We gaan ons ngram script toepassen op de publicaties: \"Cancer: Its Cause and Treatment\" volume I & II van Lucius Duncan Bulkley. Om dit te doen moet de inhoud van de teksten eerst gedownload worden en worden opgeslagen als .txt bestand.",
   "id": "f34f3c75894045e1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Vervolgens kan onze tokenizer worden aangeroepen om de tekst te veranderen in tokens. Hierbij wordt eerst de 'learn' command aangeroepen, waarbij hij wordt getraind op de meegegeven text en BPE uitvoert met de gegeven waarden voor max_tokens en min_freq. Vervolgens wordt de 'tokenize' command uitgevoerd, deze zet de meegegeven tekst om in tokens. Aan het learn command geef ik de tekst mee en de waarden voor max_tokens (1000) en min_freq (15). Er mogen dus maximaal 1000 tokens worden aangemaakt en de tokens die worden aangemaakt moeten minimaal 15 keer voorkomen in de tekst.\n",
   "id": "9ca0d9a28e9e2c99"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T17:13:24.139118Z",
     "start_time": "2025-12-13T17:12:37.604971Z"
    }
   },
   "cell_type": "code",
   "source": "!python tokenizer.py learn -i .\\resources\\gutenberg_cancer.txt -t 1000 -f 15",
   "id": "7c426a15c941eeef",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding saved: C:\\Users\\yamil\\OneDrive - Hanze\\Bio-informatica\\Jaar 3\\3.3 Modelling Cancer\\natural-language-processing\\gutenberg_cancer.enc\n",
      "BPE learned! Max tokens respected: 1000\n"
     ]
    }
   ],
   "execution_count": 92
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Hieronder zijn alle aangemaakte tokens te zien:",
   "id": "977d86906f11f269"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T17:14:37.746911Z",
     "start_time": "2025-12-13T17:14:37.742443Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(r\"gutenberg_cancer.enc\", \"r\", encoding=\"utf-8\") as f:\n",
    "    contents = f.read()\n",
    "    print(contents)"
   ],
   "id": "a430ffeaba330576",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1:t\n",
      "2:h\n",
      "3:e\n",
      "4:p\n",
      "5:r\n",
      "6:o\n",
      "7:j\n",
      "8:c\n",
      "9:g\n",
      "10:u\n",
      "11:n\n",
      "12:b\n",
      "13:k\n",
      "14:f\n",
      "15:a\n",
      "16::\n",
      "17:i\n",
      "18:s\n",
      "19:d\n",
      "20:m\n",
      "21:,\n",
      "22:v\n",
      "23:l\n",
      "24:1\n",
      "25:(\n",
      "26:2\n",
      "27:)\n",
      "28:y\n",
      "29:w\n",
      "30:.\n",
      "31:-\n",
      "32:9\n",
      "33:0\n",
      "34:[\n",
      "35:#\n",
      "36:5\n",
      "37:]\n",
      "38:/\n",
      "39:*\n",
      "40:_\n",
      "41:;\n",
      "42:x\n",
      "43:q\n",
      "44:z\n",
      "45:3\n",
      "46:4\n",
      "47:7\n",
      "48:6\n",
      "49:“\n",
      "50:”\n",
      "51:ö\n",
      "52:!\n",
      "53:?\n",
      "54:8\n",
      "55:—\n",
      "56:é\n",
      "57:½\n",
      "58:’\n",
      "59:‒\n",
      "60:æ\n",
      "61:ü\n",
      "62:œ\n",
      "63:}\n",
      "64:ë\n",
      "65:℞\n",
      "66:‾\n",
      "67:¼\n",
      "68:℥\n",
      "69:ʒ\n",
      "70:¾\n",
      "71:+\n",
      "72:%\n",
      "73:$\n",
      "74:™\n",
      "75:•\n",
      "76:‘\n",
      "77:〃\n",
      "78:×\n",
      "79:=\n",
      "80:ô\n",
      "81:ℨ\n",
      "82:ā\n",
      "83:th\n",
      "84:er\n",
      "85:he\n",
      "86:in\n",
      "87:an\n",
      "88:re\n",
      "89:at\n",
      "90:on\n",
      "91:en\n",
      "92:ti\n",
      "93:nd\n",
      "94:of\n",
      "95:te\n",
      "96:es\n",
      "97:or\n",
      "98:ce\n",
      "99:al\n",
      "100:ca\n",
      "101:ed\n",
      "102:se\n",
      "103:is\n",
      "104:it\n",
      "105:as\n",
      "106:ar\n",
      "107:ea\n",
      "108:st\n",
      "109:nt\n",
      "110:nc\n",
      "111:ha\n",
      "112:me\n",
      "113:ve\n",
      "114:io\n",
      "115:ic\n",
      "116:to\n",
      "117:di\n",
      "118:ro\n",
      "119:ng\n",
      "120:ou\n",
      "121:le\n",
      "122:ta\n",
      "123:be\n",
      "124:co\n",
      "125:li\n",
      "126:ma\n",
      "127:de\n",
      "128:ra\n",
      "129:ly\n",
      "130:hi\n",
      "131:ur\n",
      "132:ec\n",
      "133:ri\n",
      "134:ne\n",
      "135:ct\n",
      "136:ch\n",
      "137:ll\n",
      "138:pe\n",
      "139:om\n",
      "140:pr\n",
      "141:el\n",
      "142:wh\n",
      "143:no\n",
      "144:la\n",
      "145:fo\n",
      "146:ut\n",
      "147:si\n",
      "148:us\n",
      "149:mo\n",
      "150:ns\n",
      "151:tr\n",
      "152:et\n",
      "153:ho\n",
      "154:ie\n",
      "155:so\n",
      "156:su\n",
      "157:ge\n",
      "158:wi\n",
      "159:ee\n",
      "160:em\n",
      "161:ot\n",
      "162:ul\n",
      "163:lo\n",
      "164:ac\n",
      "165:ss\n",
      "166:rt\n",
      "167:un\n",
      "168:ts\n",
      "169:il\n",
      "170:rs\n",
      "171:ry\n",
      "172:ab\n",
      "173:op\n",
      "174:we\n",
      "175:ai\n",
      "176:tu\n",
      "177:im\n",
      "178:ad\n",
      "179:ni\n",
      "180:iv\n",
      "181:na\n",
      "182:os\n",
      "183:fe\n",
      "184:ow\n",
      "185:po\n",
      "186:bl\n",
      "187:ol\n",
      "188:ci\n",
      "189:ev\n",
      "190:pa\n",
      "191:av\n",
      "192:id\n",
      "193:rg\n",
      "194:mi\n",
      "195:sh\n",
      "196:fr\n",
      "197:od\n",
      "198:bo\n",
      "199:ef\n",
      "200:vi\n",
      "201:rm\n",
      "202:gr\n",
      "203:fi\n",
      "204:cu\n",
      "205:wa\n",
      "206:mp\n",
      "207:ia\n",
      "208:ig\n",
      "209:ty\n",
      "210:bu\n",
      "211:ex\n",
      "212:ir\n",
      "213:ag\n",
      "214:uc\n",
      "215:if\n",
      "216:um\n",
      "217:ls\n",
      "218:sa\n",
      "219:lt\n",
      "220:rd\n",
      "221:ay\n",
      "222:ue\n",
      "223:va\n",
      "224:gh\n",
      "225:by\n",
      "226:ny\n",
      "227:lu\n",
      "228:ep\n",
      "229:oc\n",
      "230:am\n",
      "231:oo\n",
      "232:wo\n",
      "233:pl\n",
      "234:cr\n",
      "235:eg\n",
      "236:ga\n",
      "237:tt\n",
      "238:cl\n",
      "239:gi\n",
      "240:yo\n",
      "241:ff\n",
      "242:ei\n",
      "243:do\n",
      "244:rr\n",
      "245:du\n",
      "246:fu\n",
      "247:ov\n",
      "248:ld\n",
      "249:fa\n",
      "250:au\n",
      "251:rk\n",
      "252:sp\n",
      "253:tl\n",
      "254:qu\n",
      "255:ap\n",
      "256:ke\n",
      "257:cc\n",
      "258:nu\n",
      "259:pi\n",
      "260:br\n",
      "261:ye\n",
      "262:ib\n",
      "263:ph\n",
      "264:ft\n",
      "265:mu\n",
      "266:gu\n",
      "267:da\n",
      "268:ob\n",
      "269:rc\n",
      "270:ug\n",
      "271:og\n",
      "272:ew\n",
      "273:rn\n",
      "274:ds\n",
      "275:ud\n",
      "276:ys\n",
      "277:up\n",
      "278:ht\n",
      "279:nl\n",
      "280:tm\n",
      "281:pp\n",
      "282:dy\n",
      "283:sc\n",
      "284:ub\n",
      "285:af\n",
      "286:pt\n",
      "287:hy\n",
      "288:je\n",
      "289:sm\n",
      "290:gl\n",
      "291:ua\n",
      "292:gn\n",
      "293:ru\n",
      "294:bs\n",
      "295:rv\n",
      "296:nn\n",
      "297:ki\n",
      "298:bi\n",
      "299:nf\n",
      "300:rl\n",
      "301:ui\n",
      "302:ak\n",
      "303:nb\n",
      "304:ba\n",
      "305:vo\n",
      "306:ms\n",
      "307:ey\n",
      "308:eo\n",
      "309:pu\n",
      "310:oj\n",
      "311:rf\n",
      "312:iz\n",
      "313:xp\n",
      "314:eq\n",
      "315:fl\n",
      "316:go\n",
      "317:xc\n",
      "318:oi\n",
      "319:wn\n",
      "320:mb\n",
      "321:kn\n",
      "322:hr\n",
      "323:xi\n",
      "324:hu\n",
      "325:ze\n",
      "326:ks\n",
      "327:ck\n",
      "328:dr\n",
      "329:my\n",
      "330:tw\n",
      "331:rb\n",
      "332:mm\n",
      "333:cy\n",
      "334:hs\n",
      "335:sy\n",
      "336:xt\n",
      "337:cs\n",
      "338:ok\n",
      "339:ip\n",
      "340:sk\n",
      "341:nv\n",
      "342:dl\n",
      "343:iu\n",
      "344:yi\n",
      "345:ju\n",
      "346:aw\n",
      "347:eb\n",
      "348:py\n",
      "349:sl\n",
      "350:oe\n",
      "351:ws\n",
      "352:tc\n",
      "353:dv\n",
      "354:lf\n",
      "355:dd\n",
      "356:yr\n",
      "357:ek\n",
      "358:bt\n",
      "359:lm\n",
      "360:wr\n",
      "361:ax\n",
      "362:nk\n",
      "363:rp\n",
      "364:wt\n",
      "365:ym\n",
      "366:ix\n",
      "367:ox\n",
      "368:bj\n",
      "369:sf\n",
      "370:ii\n",
      "371:lc\n",
      "372:yt\n",
      "373:eu\n",
      "374:ps\n",
      "375:jo\n",
      "376:ka\n",
      "377:gs\n",
      "378:xa\n",
      "379:yp\n",
      "380:oy\n",
      "381:dg\n",
      "382:lv\n",
      "383:lp\n",
      "384:uf\n",
      "385:ik\n",
      "386:pm\n",
      "387:uo\n",
      "388:gy\n",
      "389:oh\n",
      "390:za\n",
      "391:ww\n",
      "392:lk\n",
      "393:wl\n",
      "394:lr\n",
      "395:yl\n",
      "396:oa\n",
      "397:rw\n",
      "398:xe\n",
      "399:rh\n",
      "400:hl\n",
      "401:kl\n",
      "402:lw\n",
      "403:gg\n",
      "404:fy\n",
      "405:gm\n",
      "406:ja\n",
      "407:nj\n",
      "408:yn\n",
      "409:hf\n",
      "410:sw\n",
      "411:tn\n",
      "412:xx\n",
      "413:yg\n",
      "414:hn\n",
      "415:dn\n",
      "416:gt\n",
      "417:xh\n",
      "418:dm\n",
      "419:nh\n",
      "420:ah\n",
      "421:nq\n",
      "422:æm\n",
      "423:ya\n",
      "424:zi\n",
      "425:bn\n",
      "426:mr\n",
      "427:aj\n",
      "428:yd\n",
      "429:eh\n",
      "430:hæ\n",
      "431:the\n",
      "432:and\n",
      "433:can\n",
      "434:cer\n",
      "435:anc\n",
      "436:ing\n",
      "437:ati\n",
      "438:ent\n",
      "439:ion\n",
      "440:tion\n",
      "441:that\n",
      "442:pro\n",
      "443:for\n",
      "444:con\n",
      "445:with\n",
      "446:ther\n",
      "447:per\n",
      "448:ich\n",
      "449:whic\n",
      "450:ere\n",
      "451:men\n",
      "452:ted\n",
      "453:ons\n",
      "454:ver\n",
      "455:have\n",
      "456:rom\n",
      "457:reat\n",
      "458:fro\n",
      "459:any\n",
      "460:this\n",
      "461:ter\n",
      "462:are\n",
      "463:ical\n",
      "464:has\n",
      "465:all\n",
      "466:ate\n",
      "467:ery\n",
      "468:but\n",
      "469:ity\n",
      "470:not\n",
      "471:ect\n",
      "472:been\n",
      "473:res\n",
      "474:pre\n",
      "475:man\n",
      "476:ase\n",
      "477:cre\n",
      "478:dis\n",
      "479:was\n",
      "480:orm\n",
      "481:ess\n",
      "482:med\n",
      "483:dise\n",
      "484:some\n",
      "485:lat\n",
      "486:erat\n",
      "487:ort\n",
      "488:ure\n",
      "489:reas\n",
      "490:ong\n",
      "491:ten\n",
      "492:mor\n",
      "493:oth\n",
      "494:you\n",
      "495:ber\n",
      "496:one\n",
      "497:inc\n",
      "498:stat\n",
      "499:ast\n",
      "500:der\n",
      "501:ers\n",
      "502:tre\n",
      "503:ated\n",
      "504:ses\n",
      "505:ents\n",
      "506:gre\n",
      "507:ary\n",
      "508:ien\n",
      "509:sea\n",
      "510:ence\n",
      "511:sur\n",
      "512:ous\n",
      "513:tal\n",
      "514:mal\n",
      "515:thes\n",
      "516:more\n",
      "517:ard\n",
      "518:cas\n",
      "519:oper\n",
      "520:ally\n",
      "521:wor\n",
      "522:out\n",
      "523:atm\n",
      "524:und\n",
      "525:tain\n",
      "526:ese\n",
      "527:ist\n",
      "528:also\n",
      "529:cen\n",
      "530:gen\n",
      "531:pat\n",
      "532:rec\n",
      "533:its\n",
      "534:able\n",
      "535:ant\n",
      "536:por\n",
      "537:year\n",
      "538:our\n",
      "539:may\n",
      "540:lect\n",
      "541:ars\n",
      "542:ound\n",
      "543:edic\n",
      "544:ould\n",
      "545:cal\n",
      "546:ance\n",
      "547:ble\n",
      "548:red\n",
      "549:use\n",
      "550:had\n",
      "551:cur\n",
      "552:par\n",
      "553:nder\n",
      "554:ser\n",
      "555:sen\n",
      "556:ctu\n",
      "557:cell\n",
      "558:reg\n",
      "559:les\n",
      "560:diet\n",
      "561:ite\n",
      "562:ork\n",
      "563:min\n",
      "564:erg\n",
      "565:enb\n",
      "566:cou\n",
      "567:ght\n",
      "568:ces\n",
      "569:gut\n",
      "570:uten\n",
      "571:thou\n",
      "572:fou\n",
      "573:alit\n",
      "574:ates\n",
      "575:tin\n",
      "576:din\n",
      "577:ful\n",
      "578:est\n",
      "579:urg\n",
      "580:enti\n",
      "581:gar\n",
      "582:mat\n",
      "583:ord\n",
      "584:jec\n",
      "585:fec\n",
      "586:ine\n",
      "587:imp\n",
      "588:rel\n",
      "589:ious\n",
      "590:rat\n",
      "591:resu\n",
      "592:only\n",
      "593:ater\n",
      "594:roj\n",
      "595:time\n",
      "596:tic\n",
      "597:int\n",
      "598:most\n",
      "599:will\n",
      "600:atis\n",
      "601:end\n",
      "602:bre\n",
      "603:whe\n",
      "604:rep\n",
      "605:dea\n",
      "606:tly\n",
      "607:tab\n",
      "608:wer\n",
      "609:who\n",
      "610:cor\n",
      "611:ele\n",
      "612:fore\n",
      "613:hen\n",
      "614:erta\n",
      "615:sul\n",
      "616:att\n",
      "617:erv\n",
      "618:she\n",
      "619:sho\n",
      "620:acti\n",
      "621:sion\n",
      "622:ough\n",
      "623:ind\n",
      "624:ism\n",
      "625:mon\n",
      "626:nor\n",
      "627:ced\n",
      "628:lon\n",
      "629:cau\n",
      "630:her\n",
      "631:blo\n",
      "632:erf\n",
      "633:eath\n",
      "634:che\n",
      "635:exp\n",
      "636:fter\n",
      "637:aft\n",
      "638:str\n",
      "639:cti\n",
      "640:urin\n",
      "641:ded\n",
      "642:lood\n",
      "643:lls\n",
      "644:ass\n",
      "645:rop\n",
      "646:ont\n",
      "647:than\n",
      "648:comp\n",
      "649:stan\n",
      "650:pos\n",
      "651:urre\n",
      "652:ref\n",
      "653:ial\n",
      "654:eral\n",
      "655:tri\n",
      "656:alth\n",
      "657:lem\n",
      "658:rea\n",
      "659:inf\n",
      "660:even\n",
      "661:ree\n",
      "662:sec\n",
      "663:gic\n",
      "664:ive\n",
      "665:ari\n",
      "666:atin\n",
      "667:ined\n",
      "668:ans\n",
      "669:ern\n",
      "670:sti\n",
      "671:obs\n",
      "672:whi\n",
      "673:vin\n",
      "674:ered\n",
      "675:kno\n",
      "676:ved\n",
      "677:ron\n",
      "678:lts\n",
      "679:var\n",
      "680:tis\n",
      "681:mar\n",
      "682:over\n",
      "683:nec\n",
      "684:they\n",
      "685:clu\n",
      "686:loc\n",
      "687:ign\n",
      "688:ased\n",
      "689:how\n",
      "690:tho\n",
      "691:diti\n",
      "692:ose\n",
      "693:tive\n",
      "694:val\n",
      "695:fre\n",
      "696:now\n",
      "697:ths\n",
      "698:ssu\n",
      "699:ount\n",
      "700:much\n",
      "701:isti\n",
      "702:meta\n",
      "703:olis\n",
      "704:qui\n",
      "705:lin\n",
      "706:ici\n",
      "707:new\n",
      "708:same\n",
      "709:sib\n",
      "710:bol\n",
      "711:ever\n",
      "712:tim\n",
      "713:ang\n",
      "714:ossi\n",
      "715:tum\n",
      "716:fin\n",
      "717:ory\n",
      "718:etar\n",
      "719:pla\n",
      "720:such\n",
      "721:tur\n",
      "722:erm\n",
      "723:ecti\n",
      "724:ger\n",
      "725:exc\n",
      "726:ener\n",
      "727:rev\n",
      "728:sid\n",
      "729:gro\n",
      "730:requ\n",
      "731:alig\n",
      "732:arly\n",
      "733:must\n",
      "734:ondi\n",
      "735:chan\n",
      "736:hed\n",
      "737:pec\n",
      "738:lar\n",
      "739:life\n",
      "740:nan\n",
      "741:cure\n",
      "742:ors\n",
      "743:lit\n",
      "744:arge\n",
      "745:onsi\n",
      "746:onal\n",
      "747:tics\n",
      "748:rem\n",
      "749:art\n",
      "750:esti\n",
      "751:thin\n",
      "752:befo\n",
      "753:well\n",
      "754:liv\n",
      "755:des\n",
      "756:hile\n",
      "757:eir\n",
      "758:remo\n",
      "759:diff\n",
      "760:inal\n",
      "761:caus\n",
      "762:act\n",
      "763:nat\n",
      "764:seen\n",
      "765:deve\n",
      "766:velo\n",
      "767:abou\n",
      "768:rog\n",
      "769:eri\n",
      "770:ear\n",
      "771:ges\n",
      "772:ital\n",
      "773:fir\n",
      "774:rst\n",
      "775:icat\n",
      "776:ark\n",
      "777:care\n",
      "778:ulat\n",
      "779:wou\n",
      "780:ged\n",
      "781:ual\n",
      "782:fer\n",
      "783:eren\n",
      "784:don\n",
      "785:main\n",
      "786:eff\n",
      "787:hosp\n",
      "788:oft\n",
      "789:mov\n",
      "790:cul\n",
      "791:wher\n",
      "792:what\n",
      "793:tan\n",
      "794:stud\n",
      "795:tra\n",
      "796:nut\n",
      "797:riti\n",
      "798:low\n",
      "799:age\n",
      "800:bod\n",
      "801:ody\n",
      "802:ian\n",
      "803:udy\n",
      "804:son\n",
      "805:row\n",
      "806:sin\n",
      "807:made\n",
      "808:kin\n",
      "809:rac\n",
      "810:shou\n",
      "811:mic\n",
      "812:utri\n",
      "813:read\n",
      "814:righ\n",
      "815:mer\n",
      "816:ren\n",
      "817:case\n",
      "818:acc\n",
      "819:uni\n",
      "820:lud\n",
      "821:flu\n",
      "822:ular\n",
      "823:vat\n",
      "824:ully\n",
      "825:ffer\n",
      "826:fic\n",
      "827:def\n",
      "828:onic\n",
      "829:fact\n",
      "830:pra\n",
      "831:mis\n",
      "832:rote\n",
      "833:ain\n",
      "834:mes\n",
      "835:org\n",
      "836:har\n",
      "837:quen\n",
      "838:ties\n",
      "839:ste\n",
      "840:skin\n",
      "841:itt\n",
      "842:adv\n",
      "843:den\n",
      "844:tem\n",
      "845:tle\n",
      "846:clin\n",
      "847:inv\n",
      "848:ines\n",
      "849:num\n",
      "850:glan\n",
      "851:aff\n",
      "852:tro\n",
      "853:sys\n",
      "854:yste\n",
      "855:als\n",
      "856:rou\n",
      "857:onst\n",
      "858:two\n",
      "859:reti\n",
      "860:llow\n",
      "861:ases\n",
      "862:them\n",
      "863:aga\n",
      "864:tea\n",
      "865:sue\n",
      "866:char\n",
      "867:inte\n",
      "868:rob\n",
      "869:oma\n",
      "870:off\n",
      "871:vol\n",
      "872:app\n",
      "873:gan\n",
      "874:seem\n",
      "875:foll\n",
      "876:giv\n",
      "877:nce\n",
      "878:heal\n",
      "879:arti\n",
      "880:ange\n",
      "881:dem\n",
      "882:ies\n",
      "883:esi\n",
      "884:real\n",
      "885:com\n",
      "886:ucti\n",
      "887:ven\n",
      "888:meas\n",
      "889:asu\n",
      "890:inst\n",
      "891:tas\n",
      "892:ret\n",
      "893:copy\n",
      "894:ishe\n",
      "895:sis\n",
      "896:alon\n",
      "897:erab\n",
      "898:occ\n",
      "899:eci\n",
      "900:get\n",
      "901:ribu\n",
      "902:ins\n",
      "903:luen\n",
      "904:sal\n",
      "905:urge\n",
      "906:nit\n",
      "907:erou\n",
      "908:dic\n",
      "909:mas\n",
      "910:ario\n",
      "911:nin\n",
      "912:dily\n",
      "913:cat\n",
      "914:eran\n",
      "915:howe\n",
      "916:wev\n",
      "917:orat\n",
      "918:comm\n",
      "919:sev\n",
      "920:lic\n",
      "921:onn\n",
      "922:inu\n",
      "923:spec\n",
      "924:sar\n",
      "925:dep\n",
      "926:agre\n",
      "927:ady\n",
      "928:geon\n",
      "929:van\n",
      "930:rect\n",
      "931:emen\n",
      "932:rodu\n",
      "933:inic\n",
      "934:vit\n",
      "935:seas\n",
      "936:ext\n",
      "937:dur\n",
      "938:wth\n",
      "939:bor\n",
      "940:ner\n",
      "941:duc\n",
      "942:orks\n",
      "943:lab\n",
      "944:ator\n",
      "945:stu\n",
      "946:day\n",
      "947:car\n",
      "948:umb\n",
      "949:iden\n",
      "950:ugh\n",
      "951:thus\n",
      "952:tes\n",
      "953:acid\n",
      "954:spit\n",
      "955:lear\n",
      "956:then\n",
      "957:ken\n",
      "958:ili\n",
      "959:appe\n",
      "960:tat\n",
      "961:err\n",
      "962:urb\n",
      "963:ith\n",
      "964:bri\n",
      "965:thy\n",
      "966:edi\n",
      "967:adi\n",
      "968:almo\n",
      "969:duce\n",
      "970:udi\n",
      "971:ress\n",
      "972:subj\n",
      "973:bjec\n",
      "974:ond\n",
      "975:rse\n",
      "976:tak\n",
      "977:far\n",
      "978:ency\n",
      "979:ire\n",
      "980:atu\n",
      "981:rent\n",
      "982:bein\n",
      "983:orti\n",
      "984:lop\n",
      "985:enou\n",
      "986:ales\n",
      "987:thre\n",
      "988:pri\n",
      "989:fur\n",
      "990:epi\n",
      "991:amon\n",
      "992:zed\n",
      "993:gin\n",
      "994:come\n",
      "995:nal\n",
      "996:itro\n",
      "997:yor\n",
      "998:etc\n",
      "999:his\n",
      "1000:ves\n",
      "\n"
     ]
    }
   ],
   "execution_count": 96
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Vervolgens zet ik de Gutenberg tekst om naar het geencodeerde format, aan de hand van de aangemaakte tokens.",
   "id": "31664c5933807b3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T17:18:18.952283Z",
     "start_time": "2025-12-13T17:18:18.065315Z"
    }
   },
   "cell_type": "code",
   "source": "!python tokenizer.py tokenize -i resources\\gutenberg_cancer.txt -e .\\gutenberg_cancer.enc",
   "id": "833b150f8b0c54ff",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens saved: C:\\Users\\yamil\\OneDrive - Hanze\\Bio-informatica\\Jaar 3\\3.3 Modelling Cancer\\natural-language-processing\\gutenberg_cancer.tok\n"
     ]
    }
   ],
   "execution_count": 103
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Het tok bestand bevat dus de geencodeerde versie van het inputbestand, hieronder is te zien hoe dat eruit ziet. Elke regel staat voor 1 woord, en elk woord bestaat uit 1 of meerdere tokens.",
   "id": "456bbf4e73f089d9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T17:44:29.731996Z",
     "start_time": "2025-12-13T17:44:29.726135Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(r\"gutenberg_cancer.tok\", \"r\", encoding=\"utf-8\") as f:\n",
    "    contents = f.read()\n",
    "    print(contents[1:100])"
   ],
   "id": "89c6920d34421db4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n",
      "442 584 1\n",
      "569 565 564\n",
      "347 231 13\n",
      "94\n",
      "433 434 16\n",
      "533\n",
      "761 3\n",
      "432\n",
      "502 523 438 21\n",
      "871 216 3\n",
      "24\n",
      "25 94\n",
      "2\n"
     ]
    }
   ],
   "execution_count": 128
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We bepalen de waarschijnlijkheden van het opvolgen van een woord op een ngram met onderstaande code. Voor unigrams (setjes van telkens 1 woord) gaat dit heel simpel, het kiest \"random\" een woord, met als bijbehorende weights hoe vaak dit woord voorkomt in de tekst. Voor hogere vormen van ngrams, bijvoorbeeld bigrams en trigrams, werkt dit iets anders. Hier is er eerst bepaald welke tokens kunnen volgen op de huidige token(s). De bigram \"het kind\" kan bijvoorbeeld opgevolgd worden door de woorden \"eet\", \"slaapt\" of \"huilt\", bij de tokens is het ook zo dat elke token of verzameling van tokens specifieke woorden heeft waardoor het opgevolgd kan worden (althans, volgens de tekst waarop hij is getraind!). Ook hier weer geldt dat er \"willekeurig\" een van de mogelijkheden wordt gekozen, met daarbij ook nog de invloed van weights, die vaak voorkomende combinaties waarschijnlijker zullen maken om gekozen te worden voor de te genereren tekst.\n",
    "\n",
    "De gegenereerde tekst wordt met behulp van de decoder van de tokenizer gedecodeerd. De output wordt opgeslagen in een output bestand. Aan de hand van het getal dat wordt gebruikt voor n, zal de output er anders uitzien. Hieronder worden teksten van 100 woorden gegenereerd, met telkens een andere waarde voor n."
   ],
   "id": "52bede05af7f43e3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T17:19:58.808612Z",
     "start_time": "2025-12-13T17:19:58.088975Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# n = 1\n",
    "!python ngram.py .\\gutenberg_cancer.tok -e .\\gutenberg_cancer.enc -n 1 -l 100 -o n1gram.txt"
   ],
   "id": "da3b8ac50f3ec5a4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-gram tekst met n:1 en lengte 100 succesvol gegenereerd, opgeslagen op n1gram.txt\n"
     ]
    }
   ],
   "execution_count": 112
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T17:20:05.027487Z",
     "start_time": "2025-12-13T17:20:04.216522Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# n = 2\n",
    "!python ngram.py .\\gutenberg_cancer.tok -e .\\gutenberg_cancer.enc -n 2 -l 100 -o n2gram.txt"
   ],
   "id": "90bde94e56e890f2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-gram tekst met n:2 en lengte 100 succesvol gegenereerd, opgeslagen op n2gram.txt\n"
     ]
    }
   ],
   "execution_count": 113
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T17:20:07.908862Z",
     "start_time": "2025-12-13T17:20:06.982100Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# n = 3\n",
    "!python ngram.py .\\gutenberg_cancer.tok -e .\\gutenberg_cancer.enc -n 3 -l 100 -o n3gram.txt"
   ],
   "id": "31ca6d5985b26c87",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-gram tekst met n:3 en lengte 100 succesvol gegenereerd, opgeslagen op n3gram.txt\n"
     ]
    }
   ],
   "execution_count": 114
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T17:20:33.463667Z",
     "start_time": "2025-12-13T17:20:33.458376Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(r\"n1gram.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    contents = f.read()\n",
    "    print(contents)"
   ],
   "id": "36ec37ccee507318",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g j c . of y om s ar dise be or ly sion reti acti are ing not cer now y l has 4 e cat , ; comp red rou ati ect ia and ved been per of oth a tt inv is und t ing . urre ted ty th 1 for was 2 , in le ter isti tum por e su pe rac thre dis ri mo it ect a has xt a . in al si ia tum dise ke have ow do thou gy ty dep ant bl can ity le all ul out , dise\n"
     ]
    }
   ],
   "execution_count": 117
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T17:20:40.458975Z",
     "start_time": "2025-12-13T17:20:40.454850Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(r\"n2gram.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    contents = f.read()\n",
    "    print(contents)"
   ],
   "id": "fbfca81ab6d7e055",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all . if you pa id for it by sen din g sc al e of can cer , as they pre sen tin g the lin e of can cer . in a case . the or ig inal ref eren ces to pro vi de a rep ort ed fro m nor mal , oft en ver y diff ic ul t to ke ep up a pro duc ts , whic h are exc lud ed . the ly mp ho cy to pro vi din g the lin es la id do wn by my vol\n"
     ]
    }
   ],
   "execution_count": 118
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-13T17:20:41.979491Z",
     "start_time": "2025-12-13T17:20:41.975069Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open(r\"n3gram.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    contents = f.read()\n",
    "    print(contents)"
   ],
   "id": "a0e7a87cf144bc9d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 1 4 , we may ci te a fe w of the case s wer e lear ne d reg ard ing the cell s to res ist the at hr ep tic inf luen ce sur gic al oper ati on the ory of ha nd le y , a . m . was fir st con sc ious of a lu mp in the bre ast , ut er us , st oma ch , inte sti nal exc reti on , who se imp ro ve at on ce cl ass them with the var ious\n"
     ]
    }
   ],
   "execution_count": 119
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Als we nu kijken naar de 3 verschillende outputs, lijkt het gelijk al duidelijk dat 1 geen goede keuze is voor een ngram. Alle tokens zijn willekeurig gekozen, dus zit er geen logica in. Veel tokens die naast elkaar zijn geplaatst, zouden normaal niet naast elkaar kunnen staan in het Engels. Bij n=2 zit hier al verbetering in. De woorden die naast elkaar staan lijken elke keer wel logisch en lijken te kloppen, maar als je dan verder kijkt dan naar de woorden direct naast elkaar, dus naar de hele zin, dan zie je dat er weinig logica in zit en de zin nog steeds niet helemaal te volgen is. Bij n=3 geldt hetzelfde als bij n=2, alleen zijn de \"logische\" stukjes dan iets groter, maar nogmaals bij het kijken naar de gehele zin lijkt het niet helemaal te kloppen. Een groot deel van hoe logisch/kloppend de zinnen worden met het genereren van tekst hangt naast de n ook af van het maximaal gekozen aantal tokens. Als er geen limiet wordt ingesteld, zal elk woord een eigen token worden en zal de tekst vanzelf ook beter te volgen zijn.",
   "id": "60870267f41628e8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
